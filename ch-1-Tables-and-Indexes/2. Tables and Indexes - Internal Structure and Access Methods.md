- [Tables and Indexes - Internal Structure and Access Methods](#tables-and-indexes---internal-structure-and-access-methods)
  - [Heap Tables](#heap-tables)
  - [Clustered Indexes](#clustered-indexes)
    - [Ordered scan](#ordered-scan)
    - [Allocation order scan](#allocation-order-scan)
    - [Index seek](#index-seek)
    - [SARGable](#sargable)
  - [Composit Indexes](#composit-indexes)

<br>

# Tables and Indexes - Internal Structure and Access Methods

## Heap Tables
Heap tables are tables without a clustered index. The data in heap tables is unsorted. ***SQL Server does not guarantee, nor does it maintain, a sorting order of the data in heap tables.***

When inserting data into heep tables, Sql server uses the page free space (PFS) allocation map. it errs on the side of caution and uses the low value from the PFS free space percentage tier during the estimation.  
For example, if a data page stores 4,100 bytes of data, and as result it has 3,960 bytes of free space available, PFS would indicate that the page is [51–80 percent full](1.%20Data%20Storage%20Internals.md#page-free-space-pfs). SQL Server would not put a new row on the page if its size exceeds 20 percent (8,060 bytes * 0.2 = 1,612 bytes) of the page size.  
This behavior leads to the situation where SQL Server unnecessarily allocates new data pages, leaving large amounts of free space unused. It is not always a problem when the size of rows vary—in those cases, SQL Server eventually fills empty spaces with the smaller rows. However, especially in cases when all rows are relatively large, you can end up with large amounts of unused space on the data pages.

When selecting data from the heap table, SQL Server uses an index allocation map (IAM) to find the pages and extents that need to be scanned. It analyzes what extents belong to the table and processes them based on their allocation order rather than on the order in which the data was inserted.

When you update a row in the heap table, SQL Server tries to accommodate it on the same page. If there is no free space available, SQL Server moves the new version of the row to another page and replaces the old row with a special 16-byte row called a ***forwarding pointer***. The new version of the row is called ***forwarded row.***

There are two main reasons why forwarding pointers are used.  
1. They prevent updates of nonclustered index keys that reference the row.
2. They help minimize the number of duplicated reads.  
   e.g. Let’s further assume that the row in page 3 was modified after the page was read at the time when SQL Server was reading page 4. The new version of the row would be moved to page 5, which has yet to be processed. Without forwarding pointers, SQL Server would not know that the old version of the row had already been read, and it would read it again during the page 5 scan. With forwarding pointers, SQL Server would ignore the forwarded rows—they have a bit set in the Status Bits A byte in the data row.

Although forwarding pointers help minimize duplicated reads, they introduce additional read operations at the same time. SQL Server follows the forwarding pointers and reads the new versions of the rows at the time it encounters them. That behavior can introduce an excessive number of I/O operations.
![Reading data when forwarding pointers exist](assets/Reading%20data%20when%20forwarding%20pointers%20exist.jpg)
When SQL Server reads the forwarding pointer rows from page 1, it follows them and reads pages 2 and 3 immediately. After that, SQL Server reads those pages one more time during the regular IAM scan process. As a result, we have five read operations, even though our table has just three data pages.

As you can see, the large number of forwarding pointers leads to extra I/O operations and significantly reduces the performance of the queries accessing the data.

When the size of the forwarded row is reduced by another update and the data page with forwarding pointer has enough space to accommodate the updated version of the row, SQL Server may move it back to its original data page and remove the forwarding pointer row. Nevertheless, the only reliable way to get rid of all of the forwarding pointers is by rebuilding the heap table.

**Heap tables can be useful in staging environments,** where you want to import a large amount of data into the system as fast as possible. Inserting data into heap tables can often be faster than inserting it into tables with clustered indexes. Nevertheless, during a regular workload, tables with clustered indexes usually outperform heap tables, which have suboptimal space control and extra I/O operations introduced by forwarding pointers.

## Clustered Indexes

A clustered index dictates the physical order of the data in a table, which is sorted according to the clustered index key. The table can have only one clustered index defined.
![Clustered index structure](assets/Clustered%20index%20structure.jpg)

> **Note**  
> The sort order on the page is controlled by a slot array. Actual data on the page is unsorted.

The number of levels in the index largely depends on the row and index key sizes. For example, the index on the 4-byte integer column will require 13 bytes per row on the intermediate and root levels. Those 13 bytes consist of a 2-byte slot-array entry, a 4-byte index-key value, a 6-byte page pointer, and a 1-byte row overhead, which is adequate because the index key does not contain variable-length and NULL columns. 

As a result, you can accommodate 8,060 bytes / 13 bytes per row = 620 rows per page. This means that, with the one intermediate level, you can store information about up to 620 * 620 = 384,400 leaf-level pages. If your data row size is 200 bytes, you can store 40 rows per leaf-level page and up to 15,376,000 rows in the index with just three levels. Adding another intermediate level to the index would essentially cover all possible integer values.

There are three different ways in which SQL Server can read data from the index.

### Ordered scan
We want to run the SELECT Name FROM dbo.Customers ORDER BY CustomerId query. The data on the leaf level of the index is already sorted based on the CustomerId column value. As a result, SQL Server can scan the leaf level of the index from the first to the last page and return the rows in the order in which they were stored.

SQL Server starts with the root page of the index and reads the first row from there. That row references the intermediate page with the minimum key value from the table. SQL Server reads that page and repeats the process until it finds the first page on the leaf level. Then, SQL Server starts to read rows one by one, moving through the linked list of the pages until all rows have been read.
![Ordered scan process](assets/Ordered%20scan%20process.jpg)

The execution plan for the preceding query shows the Clustered Index Scan operator with the ***Ordered property*** set to true,

It is worth mentioning that the order by clause is not required for an ordered scan to be triggered. An ordered scan just means that SQL Server reads the data based on the order of the index key.

SQL Server can navigate through indexes in both directions, forward and backward. However, there is one important aspect that you must keep in mind: ***SQL Server does not use parallelism during backward index scans.***

You cannot rely on the order of the index keys. ***You should always specify an ORDER BY clause when it matters.***

### Allocation order scan
SQL Server accesses the table data through the IAM pages, similar to how it does so with heap tables.
![Allocation order scan](assets/Allocation%20order%20scan.jpg)

An allocation order scan can be faster for scanning large tables, although it has a higher startup cost. SQL Server does not use this access method when the table is small. Another important consideration is data consistency. SQL Server does not use forwarding pointers in tables that have a clustered index, and an allocation order scan can produce inconsistent results. Rows can be skipped or read multiple times due to the data movement caused by page splits. As a result, SQL Server usually avoids using allocation order scans unless it reads the data in READ UNCOMMITTED or SERIALIZABLE transaction-isolation levels.

### Index seek
SELECT Name FROM dbo.Customers WHERE CustomerId BETWEEN 4 AND 7
![Index seek process](assets/Index%20seek%20process.jpg)

SQL Server starts with the root page, where the second row references the page with the minimum key value of 350. It is greater than the key value that we are looking for (4), and SQL Server reads the intermediate-level data page (1:170) referenced by the first row on the root page.   
Similarly, the intermediate page leads SQL Server to the first leaf-level page (1:176). SQL Server reads that page, then it reads the rows with CustomerIds equal to 4 and 5, and, finally, it reads the two remaining rows from the second page.

Index seek is more efficient than index scan, because SQL Server processes just the subset of rows and data pages rather than scanning the entire table.

### SARGable
There is a concept in relational databases called SARGable predicates, which stands for Search Argument able. The predicate is SARGable if SQL Server can utilize an index seek operation, if an index exists. In a nutshell, predicates are SARGable when SQL Server can isolate the single value or range of index key values to process, thus limiting the search during predicate evaluation. Obviously, it is beneficial to write queries using SARGable predicates and utilize index seek whenever possible.

SARGable predicates include the following operators: =, >, >=, <, <=, IN, BETWEEN, and LIKE (in case of prefix matching). Non-SARGable operators include NOT, <>, LIKE (in case of non-prefix matching), and NOT IN.

Another circumstance for making predicates non-SARGable is using functions or mathematical calculations against the table columns. SQL Server has to call the function or perform the calculation for every row it processes.

|Operation|Non-SARGable implementation|SARGable implementation|
|---|---|---|
|Mathematical calculations|Column - 1 = @Value|Column = @Value + 1|
|---|ABS(Column) = 1|Column IN (-1, 1)|
|Date manipulation|CAST(Column as date) = @Date (in SQL Server prior 2008) convert(datetime, convert(varchar(10),Column,121))|Column >= @Date and Column < DATEADD(day,1,@Date)|
||DATEPART(year,Column) = @Year|Column >= @Year and Column < DATEADD(year,1,@Year)|
||DATEADD(day,7,Column) > GETDATE()|Column > DATEADD(day,-7,GETDATE())|
|Prefix search|LEFT(Column,3) = 'ABC'|Column LIKE 'ABC%'|
|Substring search|Column LIKE '%ABC%'|Use Full-Text Search or other technologies|

Another important factor that you must keep in mind is type conversion. In some cases, you can make predicates non-SARGable by using incorrect data types.
![non-SARGAble data conversion](assets/non-SARGAble%20data%20conversion.jpg)
> **Tip**  
> Pay attention to the column data types in the join predicates. Implicit or explicit data type conversions can significantly decrease the performance of the queries.

You will observe very similar behavior in the case of unicode string parameters.  
e.g.
```
select * from dbo.Data where VarcharKey = '200';  
select * from dbo.Data where VarcharKey = N'200'; -- unicode parameter
```
A unicode string parameter is non-SARGable for varchar columns.  
Always specify parameter data types in client applications.

It is also worth mentioning that varchar parameters are SARGable for nvarchar unicode data columns.

## Composit Indexes
Indexes with multiple key columns are called composite (or compound) indexes. The data in the composite indexes is sorted on a per-column basis from leftmost to rightmost columns.
![composit index structure](assets/composit%20index%20structure.jpg)

